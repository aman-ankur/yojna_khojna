from pydantic import BaseModel, Field
from typing import Dict, Any, List, Optional, Tuple

# LangChain Core types for history
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage

class DocumentChunk(BaseModel):
    """
    Represents a single chunk of text extracted from a document.
    Includes metadata about the chunk's origin.
    """
    chunk_id: str = Field(..., description="Unique identifier for the chunk (e.g., {document_id}_page_{page_num}_chunk_{chunk_index})")
    document_id: str = Field(..., description="Identifier of the source document")
    text: str = Field(..., description="The actual text content of the chunk")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Metadata associated with the chunk (e.g., page number, chunk index)")
    embedding: Optional[List[float]] = Field(default=None, description="Vector embedding of the text content")

# Define a model for a single message in the history to ensure structure
# This is slightly redundant with LangChain's BaseMessage but provides Pydantic validation
# class HistoryMessage(BaseModel):
#     role: str = Field(..., description="'human' or 'ai'")
#     content: str = Field(..., description="The message content")

class ChatQuery(BaseModel):
    """Request model for the /chat endpoint."""
    question: str = Field(..., description="The user's latest question for the RAG system")
    # chat_history: List[HistoryMessage] = Field(
    #     default_factory=list,
    #     description="The conversation history as a list of messages (human/ai)"
    # )
    # Using simple tuples for now for ease of client implementation,
    # but Pydantic models (like above) are more robust.
    chat_history: List[Tuple[str, str]] = Field(
        default_factory=list,
        description="The conversation history as a list of (human_query, ai_response) tuples."
    )
    # Optional: Add fields for session_id, user_id, etc., if needed later


class ChatResponse(BaseModel):
    """Response model for the /chat endpoint."""
    answer: str = Field(..., description="The answer generated by the RAG system")
    # Returning the history is often useful for the client to maintain state,
    # especially if the server isn't storing session state itself.
    updated_history: List[Tuple[str, str]] = Field(
        default_factory=list,
        description="The updated conversation history including the latest exchange."
    )
    # Optional: Add fields for retrieved context, confidence scores, etc.

# New schemas for suggested questions
class SuggestedQuestion(BaseModel):
    id: str
    text: str

class SuggestedQuestionsRequest(BaseModel):
    question: str
    answer: str
    chat_history: List[dict] = Field(default_factory=list)  # List of message objects
    conversationId: Optional[str] = None
    lastResponseId: Optional[str] = None

class SuggestedQuestionsResponse(BaseModel):
    suggestions: List[SuggestedQuestion]

# You can add other data models (schemas) needed by your application here
# For example:
# class ProcessedDocument(BaseModel):
#     document_id: str
#     source_path: str
#     status: str
#     extracted_text: List[Tuple[int, str]] | None = None
#     chunks: List[DocumentChunk] | None = None 